library(tidyverse)
library(magrittr)
library(caret)
library(xgboost)
library(knitr)
set.seed(0)



datatrain$target = factor(datatrain$target, levels = c(1, 0),labels = c('yes', 'no'))
tri <- caret::createDataPartition(y, p = 0.9, list = F) %>% c()
optxgb <- datatrain[-tri,]

# set up the cross-validated hyper-parameter search
xgb_grid_1 = expand.grid(
  nrounds = sample(500:100000,5),
  eta = runif(5, min = 0.0000001, max = 0.3),
  max_depth = sample(1:100,5),
  gamma = runif(5, min = 0, max = 100),
  colsample_bytree = runif(5, min = 0.1, max = 1),
  min_child_weight = sample(1:100,5),
  subsample = runif(5, min = 0.1, max = 1)
)


# pack the training control parameters
xgb_trcontrol_1 = trainControl(
  method = "cv",
  number = 100,
  verboseIter = TRUE,
  returnData = FALSE,
  returnResamp = "all",                                                        # save losses across all models
  classProbs = TRUE,                                                           # set to TRUE for AUC to be computed
  summaryFunction = twoClassSummary,
  allowParallel = TRUE
)


# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(
  x = as.matrix(optxgb[,2:206]),
  y = as.factor(optxgb$target),
  trControl = xgb_trcontrol_1,
  tuneGrid = xgb_grid_1,
  method = "xgbTree"
)

# scatter plot of the AUC against max_depth and eta
ggplot(xgb_train_1$results, aes(x = as.factor(eta), y = max_depth, size = ROC, color = ROC)) +
  geom_point() +
  theme_bw() +
  scale_size_continuous(guide = "none")
